{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5 - Q-Learning Agent by Abdullah Karag√∏z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20348\\3593848235.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QLearningCalculator class has only some functions to help answer the assignment questions. The states and actions start from 1 as described in the assignment, but in the inner code they start from 0.\n",
    "\n",
    "We assume the number of states and actions are equal, where actions just describe \"which state to go from there\". Not all states are available from any state. There's no \"punishment\" for hitting walls as it was not specified in the assignment text, so the reward for hitting walls is just 0. The unavailable actions (like hitting walls) are marked as None (or nan in Numpy), but they return the reward as 0.\n",
    "\n",
    "The states 1 to 6 and actions are put into indexes from 0 to 5 on Numpy matrices. We assume we know all the available actions and states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningCalculator():\n",
    "    '''\n",
    "    This class is just calcualting some functions of Q learning algorithm. It's not a class that fully\n",
    "    implements Q learning algorithm. States and actions start from 1.\n",
    "    \n",
    "    Attributes:\n",
    "        R (Numpy matrice): Reward matrice, each row for each state, each column for each action. \n",
    "            There are equal number of actions and states.\n",
    "        Q (Numpy matrice): Q matrice\n",
    "        gamma (float): discounting factor to discount future rewards\n",
    "        lr (float): learning rate\n",
    "        state (int): the state our agent is in\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, R, nr_of_states, discount_factor, learning_rate, init_state):\n",
    "        '''\n",
    "        Initializing the QLearning Calculator class.\n",
    "        \n",
    "        Parameters:\n",
    "            R (Numpy matrice): Reward matrice, each row for each state, each column for each action.\n",
    "            nr_of_states (int): How many states and actions there should be (used to initialize Q array)\n",
    "            discount_factor (float): discounting factor to discount future rewards\n",
    "            learning_rate (float): learning rate\n",
    "            init_state (int): the state our agent is in at the start    \n",
    "        '''\n",
    "        self.R = R\n",
    "        # Q matrix. Here we know number of states.\n",
    "        self.Q = np.zeros((nr_of_states, nr_of_states), dtype=np.float64) \n",
    "        self.gamma = discount_factor\n",
    "        self.lr = learning_rate\n",
    "        self.state = init_state-1\n",
    "    \n",
    "    \n",
    "    def set_state(self, state):\n",
    "        '''\n",
    "        Function to set state of the agent.\n",
    "        \n",
    "        Parameters:\n",
    "            state (int): The state of the agent. Starts from 1.\n",
    "        '''\n",
    "        self.state = state-1\n",
    "    \n",
    "    \n",
    "    def action(self, action):\n",
    "        '''\n",
    "        Function to apply action.\n",
    "        \n",
    "        Parameters:\n",
    "            action (int): The action to take, starts from 1.\n",
    "            \n",
    "        Returns:\n",
    "            int: the next state that action put the agent into. Starts from 1.\n",
    "        '''\n",
    "        action -= 1\n",
    "        if np.isnan(R[self.state, action]):\n",
    "            q_value = 0\n",
    "            next_state = self.state\n",
    "        else:\n",
    "            next_state = action\n",
    "            q_value = (1 - self.lr)*self.Q[self.state, action] + self.lr*(R[self.state, action] + self.gamma*(np.max(self.Q[next_state])))\n",
    "        \n",
    "        self.Q[self.state][action] = q_value\n",
    "        self.state = next_state\n",
    "        return self.state\n",
    "        \n",
    "    def next_action(self, state):\n",
    "        '''\n",
    "        Function to return most probable next action, given state.\n",
    "        \n",
    "        Parameters:\n",
    "            state (int): The given state.\n",
    "            \n",
    "        Returns:\n",
    "            int: the most probable action to take given state.\n",
    "        '''\n",
    "        next_action = np.argmax(self.Q[state-1]) + 1\n",
    "        return next_action\n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing R matrice, discount factor and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward matrix\n",
    "R = np.array([[None,0,None,0,None,None], [0,None,-10,None,0,None], [None,0,None,None,None,100], \n",
    "              [0,None,None,None,0,None], [None,0,None,0,None,100], [None,None,-10,None,0,None]], dtype=np.float64)\n",
    "\n",
    "dis_fac = 0.9 # Discount factor\n",
    "lr = 1.0 # Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ql_calc = QLearningCalculator(R, len(R), dis_fac, lr, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Q matrix looks like this:\n",
      " [[  0.   0.   0.  81.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.  90.   0.]\n",
      " [  0.   0.   0.   0.   0. 100.]\n",
      " [  0.   0.   0.   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "ql_calc.action(5)\n",
    "ql_calc.action(6)\n",
    "ql_calc.set_state(4)\n",
    "ql_calc.action(5)\n",
    "ql_calc.set_state(1)\n",
    "ql_calc.action(4)\n",
    "print(\"The Q matrix looks like this:\\n\", ql_calc.Q) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the most probable path from state 2 \n",
    "Below is when we start from 2 and continue using np.max(Q values given state) when choosing next action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most probable path is: 2 --> 1 --> 4 --> 5 --> 6\n"
     ]
    }
   ],
   "source": [
    "#We start from 2\n",
    "state = 2\n",
    "ql_calc.set_state(2)\n",
    "path = str(2)\n",
    "while state != 6:\n",
    "    state = ql_calc.next_action(state)\n",
    "    ql_state = ql_calc\n",
    "    path += \" --> \" + str(state)\n",
    "    ql_calc.next_action(state)\n",
    "\n",
    "print(\"The most probable path is:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On state 2, any action has Q value zero, so any action is equally probably to choose.\n",
    "But if it chooses 1 (which is the case when you use max() or np.max() function), then it will move to 4, then 5 and then to 6.\n",
    "\n",
    "So the path will probably be like: 2 --> 1 --> 4 --> 5 --> 6\n",
    "\n",
    "But if it makes a random choice between max Q values, then other paths could be like following:\n",
    "\n",
    "If it chooses 5, it will then move to 6 directly like 2 --> 5 --> 6\n",
    "\n",
    "If it chooses 3, it will then learn that choosing 3 from 2 is bad choice. But both 2 or 6 from 3 will be equally probable.\n",
    "So then it can be like: \n",
    "\n",
    "2 --> 3 --> 6 \n",
    "\n",
    "or \n",
    "\n",
    "2 --> 3 --> 2 --> 1 --> 4 --> 5 --> 6 \n",
    "\n",
    "or \n",
    "\n",
    "2 --> 3 --> 2 --> 5 --> 6\n",
    "\n",
    "So here are the probable paths:\n",
    "\n",
    "2 --> 1 --> 4 --> 5 --> 6\n",
    "\n",
    "2 --> 5 --> 6\n",
    "\n",
    "2 --> 3 --> 6\n",
    "\n",
    "2 --> 3 --> 2 --> 1 --> 4 --> 5 --> 6\n",
    "\n",
    "2 --> 3 --> 2 --> 5 --> 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
